{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pororo import Pororo\n",
    "\n",
    "\n",
    "class UnsmileDataset(Dataset):\n",
    "    '''\n",
    "    unsmile_df(pandas.DataFrame): unsmile dataset with vectorized sentence('문장')\n",
    "    '''\n",
    "    def __init__(self, unsmile_df):\n",
    "        self.unsmile_df = unsmile_df\n",
    "      \n",
    "    def __len__(self):\n",
    "        return self.unsmile_df.shape[0]\n",
    "      \n",
    "    def __getitem__(self, index):\n",
    "        sentence_id = self.unsmile_df.iloc[index][0]\n",
    "        sentence_vector = self.unsmile_df.iloc[index][2]\n",
    "        label_vector = self.unsmile_df.iloc[index][3:].to_numpy(dtype=np.int32) # note dtype\n",
    "        \n",
    "        return {'id': sentence_id, 'x': sentence_vector, 'y': label_vector}\n",
    "    \n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    '''\n",
    "    input: 768 dimension sentence vector transformed by Pororo sentence embedding \n",
    "    output: 11 dimension vector which contains values for '여성/가족', .... , '개인지칭'\n",
    "    '''\n",
    "    input_dim = 768\n",
    "    hidden_dim = 512\n",
    "    output_dim = 11  \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.fc= nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "def prune_dataset(data_file):\n",
    "    data_dir = os.getcwd() + '/dataset/korean_unsmile_dataset/'\n",
    "    # data_dir = os.getcwd() + '/drive/MyDrive/dataset/korean_unsmile_dataset-main/'\n",
    "    df = pd.read_csv(data_dir + data_file, sep='\\t')\n",
    "\n",
    "    categories = df.columns.to_list()[1:]\n",
    "\n",
    "    for category in categories:\n",
    "        if category == 'clean':\n",
    "            continue\n",
    "        \n",
    "        indexes = list()\n",
    "        for i, _ in df.iterrows():\n",
    "            data = df.loc[i]\n",
    "            if data[category] == 1:\n",
    "                indexes.append(i)\n",
    "\n",
    "        mask = np.random.random(len(indexes)) > 0.5\n",
    "        indexes = mask * indexes\n",
    "        df.drop(indexes, inplace=True, errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def vectorize_dataset(df, vectorizer):\n",
    "    '''\n",
    "    transform '문장' column's elements from string to numpy array,\n",
    "    and return the pandas dataframe. Pororo is used for sentence embedding.\n",
    "    '''\n",
    "    \n",
    "    arr = []\n",
    "    sentence_col_idx = 2\n",
    "    sentence_col_name = df.columns.to_list()[sentence_col_idx]\n",
    "    \n",
    "    for i, _ in df.iterrows():\n",
    "        vectorized_sentence = vectorizer(df.loc[i][sentence_col_name])\n",
    "        arr.append(vectorized_sentence)\n",
    "\n",
    "    vectorized_sentence_col = pd.Series(arr, name=sentence_col_name)\n",
    "    df.drop(columns=sentence_col_name, axis=1, inplace=True) # remove a column with raw sentences\n",
    "    df.insert(sentence_col_idx, sentence_col_name, vectorized_sentence_col) # insert a new vectorized column\n",
    "    return df\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    '''\n",
    "    returns iterator for batch-size data.\n",
    "    \n",
    "    1. drop_last set to True: if the number of data is not divisible by batch size,\n",
    "    do not use the last batch whose size is smaller than batch size\n",
    "\n",
    "    2. shuffle set to True: shuffle dataset at every epoch\n",
    "    '''\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=dataset, batch_size=batch_size, drop_last=drop_last, shuffle=shuffle)\n",
    "  \n",
    "    for batch_dict in train_dataloader:\n",
    "        out_dict = {}\n",
    "        for k, v in batch_dict.items():\n",
    "            out_dict[k] = batch_dict[k].to(device)\n",
    "        yield out_dict\n",
    "\n",
    "def compute_metrics(y_pred, y_label, prev_precision):\n",
    "    '''\n",
    "    calculate precision and recall of batch-size data    \n",
    "    '''\n",
    "    \n",
    "    y_label = y_label.cpu()\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).cpu().int()\n",
    "    \n",
    "    if y_pred.sum().item() == 0:\n",
    "        precision = prev_precision\n",
    "    else:\n",
    "        precision = y_label[y_pred == 1].sum().item() / y_pred.sum().item()\n",
    "        \n",
    "    recall = y_label[y_pred == 1].sum().item() / (y_label == 1).sum().item()\n",
    "    return precision, recall\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'train_loss': [], 'test_loss': [],\n",
    "            'train_precision': [], 'test_precision': [],\n",
    "            'train_recall': [], 'test_recall': []}\n",
    "\n",
    "def count(df):\n",
    "    clean_data_num = df.sum()[-2]\n",
    "    hatred_data_num = df.sum()[1:].sum() - clean_data_num\n",
    "    print(f'hatred data: {hatred_data_num}, clean data: {clean_data_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd() + '/dataset/korean_unsmile_dataset/'\n",
    "train_data_file = 'unsmile_train_v1.0.tsv'\n",
    "test_data_file = 'unsmile_valid_v1.0.tsv'\n",
    "vectorizer = Pororo(task='sentence_embedding', lang='ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "train_df = pd.read_csv(data_dir + train_data_file, sep='\\t')\n",
    "test_df = pd.read_csv(data_dir + test_data_file, sep='\\t')\n",
    "print(len(train_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "print(len(merged_df))\n",
    "print(len(train_df) + len(test_df) == len(merged_df))\n",
    "merged_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 제거\n",
    "indexes = []\n",
    "for i in range(len(merged_df)):\n",
    "    data = merged_df.iloc[i]\n",
    "    if data[1:].sum() == 0:\n",
    "        indexes.append(i)\n",
    "merged_df.iloc[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(indexes, inplace=True)\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "print(len(merged_df))\n",
    "merged_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle, re-index and partition\n",
    "shuffled_df = merged_df.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = -1\n",
    "n = 3000\n",
    "ids = []\n",
    "partitions = []\n",
    "\n",
    "for i in range(len(shuffled_df)):\n",
    "    if i % n == 0 and p < 5:\n",
    "        p += 1\n",
    "    ids.append(i)\n",
    "    partitions.append(p)\n",
    "\n",
    "ids = pd.Series(ids, name='id')\n",
    "partitions = pd.Series(partitions, name='partition')\n",
    "\n",
    "shuffled_and_partitioned_df = pd.concat([ids, partitions, shuffled_df], axis=1)\n",
    "shuffled_and_partitioned_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df = vectorize_dataset(shuffled_and_partitioned_df.copy(), vectorizer)\n",
    "vectorized_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_and_partitioned_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    batch_size=128,\n",
    "    learning_rate=0.0005,\n",
    "    num_epochs=30,\n",
    "    cuda=False,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = None\n",
    "test_df = None\n",
    "K = 6 # number of partitions\n",
    "partitions = list(range(K))\n",
    "\n",
    "high_indexes = defaultdict(list)\n",
    "\n",
    "for i in range(K):\n",
    "    # prepare data\n",
    "    test_partition = partitions[i: i + 1]\n",
    "    train_partition = partitions[0: i] + partitions[i + 1:]\n",
    "    c1 = vectorized_df['partition'].isin(test_partition)\n",
    "    c2 = vectorized_df['partition'].isin(train_partition)\n",
    "    \n",
    "    test_df = vectorized_df.loc[c1]\n",
    "    train_df = vectorized_df.loc[c2]\n",
    "    train_data = UnsmileDataset(train_df)\n",
    "    test_data = UnsmileDataset(test_df)\n",
    "    \n",
    "    train_state = make_train_state(args)\n",
    "\n",
    "    # model\n",
    "    model = MultiLayerPerceptron()\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    # loss and optimizer\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # train starts\n",
    "    start_time = time.time()\n",
    "    print(f'\\n<training start!>')\n",
    "    print(f'-learning rate: {args.learning_rate}')\n",
    "    print(f'-total epochs: {args.num_epochs}')\n",
    "    print(f'-batch size: {args.batch_size}')\n",
    "    print(f\"-cuda {'avaialble' if args.cuda else 'not available'}\")\n",
    "\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        print(f'epoch{epoch_index + 1} : [', end='')\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_precision = 0.0\n",
    "        running_recall = 0.0\n",
    "        model.train() # this has effects on certain modules (ex, dropout)\n",
    "    \n",
    "        batch_generator = generate_batches(train_data, args.batch_size, device=args.device)\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            if batch_index % (int(int(len(train_data) / args.batch_size) / 20)) == 0:\n",
    "                print('>', end='')\n",
    "\n",
    "            x = batch_dict['x']\n",
    "            y = batch_dict['y']\n",
    "\n",
    "            # set all gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            y_pred = model(x)\n",
    "            loss = loss_func(y_pred, y.float())\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # metrics\n",
    "            loss_batch = loss.item()\n",
    "            running_loss += (loss_batch - running_loss) / (batch_index + 1) # moving average\n",
    "            batch_precision, batch_recall = compute_metrics(y_pred, y, running_precision)\n",
    "            running_precision += (batch_precision - running_precision) / (batch_index + 1)\n",
    "            running_recall += (batch_recall - running_recall) / (batch_index + 1)\n",
    "\n",
    "        print(']')\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_precision'].append(running_precision)\n",
    "        train_state['train_recall'].append(running_recall)\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        running_precision = 0.0\n",
    "        running_recall = 0.0\n",
    "        model.eval()\n",
    "\n",
    "        batch_generator = generate_batches(test_data, args.batch_size, device=args.device)\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            x = batch_dict['x']\n",
    "            y = batch_dict['y']\n",
    "            \n",
    "            # forward pass\n",
    "            y_pred = model(x)\n",
    "            loss = loss_func(y_pred, y.float())\n",
    "\n",
    "            # metrics\n",
    "            loss_batch = loss.item()\n",
    "            running_loss += (loss_batch - running_loss) / (batch_index + 1) # moving average\n",
    "            batch_precision, batch_recall = compute_metrics(y_pred, y, running_precision)\n",
    "            running_precision += (batch_precision - running_precision) / (batch_index + 1)\n",
    "            running_recall += (batch_recall - running_recall) / (batch_index + 1)\n",
    "    \n",
    "        train_state['test_loss'].append(running_loss)\n",
    "        train_state['test_precision'].append(running_precision)\n",
    "        train_state['test_recall'].append(running_recall)\n",
    "\n",
    "    print(f'time flied: {time.time() - start_time} sec')\n",
    "    print('<end training!>')\n",
    "    \n",
    "    # data re-labeling using model results\n",
    "    batch_generator = generate_batches(test_data, 1, device=args.device)\n",
    "    y_preds = []\n",
    "    y_labels = []\n",
    "    ids = []\n",
    "    threshold = 0.9\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        x = batch_dict['x']\n",
    "        y = batch_dict['y']\n",
    "        id = batch_dict['id']\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        y_pred = (torch.sigmoid(y_pred) > threshold).int()\n",
    "        y_preds.append(y_pred.detach().cpu().numpy().reshape(-1))\n",
    "        y_labels.append(y.detach().cpu().numpy().reshape(-1))\n",
    "        ids.append(id.detach().cpu().numpy().reshape(-1))\n",
    "        \n",
    "    y_preds = np.array(y_preds)\n",
    "    y_labels = np.array(y_labels)\n",
    "    ids = np.array(ids)\n",
    "    \n",
    "    for col_idx in range(11):\n",
    "        indexes = (y_preds[:, col_idx] == 1)\n",
    "        high_indexes[col_idx].extend(ids[indexes].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itocol = {0: '여성/가족', 1: '남성', 2: '성소수자', 3: '인종/국적', 4: '연령',\n",
    "          5: '지역', 6: '종교', 7: '기타 혐오', 8: '악플/욕설', 9: 'clean', 10: '개인지칭'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(f'category: {itocol[i]}')\n",
    "df = shuffled_and_partitioned_df.loc[high_indexes[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "df.loc[df[itocol[i]] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "print(f'category: {itocol[i]}')\n",
    "df = shuffled_and_partitioned_df.loc[high_indexes[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "df.loc[df[itocol[i]] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(f'category: {itocol[i]}')\n",
    "df = shuffled_and_partitioned_df.loc[high_indexes[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "df.loc[df[itocol[i]] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "print(f'category: {itocol[i]}')\n",
    "df = shuffled_and_partitioned_df.loc[high_indexes[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "df.loc[df[itocol[i]] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "print(f'category: {itocol[i]}')\n",
    "df = shuffled_and_partitioned_df.loc[high_indexes[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "df.loc[df[itocol[i]] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "print(f'category: {itocol[i]}')\n",
    "df = shuffled_and_partitioned_df.loc[high_indexes[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "df.loc[df[itocol[i]] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6\n",
    "print(f'category: {itocol[i]}')\n",
    "df = shuffled_and_partitioned_df.loc[high_indexes[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "df.loc[df[itocol[i]] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 7\n",
    "print(f'category: {itocol[i]}')\n",
    "df = shuffled_and_partitioned_df.loc[high_indexes[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "df.loc[df[itocol[i]] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "print(f'category: {itocol[i]}')\n",
    "df = shuffled_and_partitioned_df.loc[high_indexes[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "df.loc[df[itocol[i]] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9\n",
    "print(f'category: {itocol[i]}')\n",
    "df = shuffled_and_partitioned_df.loc[high_indexes[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "df.loc[df[itocol[i]] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "print(f'category: {itocol[i]}')\n",
    "df = shuffled_and_partitioned_df.loc[high_indexes[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "df.loc[df[itocol[i]] == 0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e78d20c099c367000af7aff7232dce7210d5e30c41d5c8d2d0b0121a579d2b3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
