{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pororo import Pororo\n",
    "\n",
    "\n",
    "class UnsmileDataset(Dataset):\n",
    "    '''\n",
    "    unsmile_df(pandas.DataFrame): unsmile dataset with vectorized sentence('문장')\n",
    "    '''\n",
    "    def __init__(self, unsmile_df):\n",
    "        self.unsmile_df = unsmile_df\n",
    "      \n",
    "    def __len__(self):\n",
    "        return self.unsmile_df.shape[0]\n",
    "      \n",
    "    def __getitem__(self, index):\n",
    "        sentence_id = self.unsmile_df.iloc[index][0]\n",
    "        sentence_vector = self.unsmile_df.iloc[index][2]\n",
    "        label_vector = self.unsmile_df.iloc[index][3:].to_numpy(dtype=np.int32) # note dtype\n",
    "        \n",
    "        return {'id': sentence_id, 'x': sentence_vector, 'y': label_vector}\n",
    "    \n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    '''\n",
    "    input: 768 dimension sentence vector transformed by Pororo sentence embedding \n",
    "    output: 11 dimension vector which contains values for '여성/가족', .... , '개인지칭'\n",
    "    '''\n",
    "    input_dim = 768\n",
    "    hidden_dim = 512\n",
    "    output_dim = 11  \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.fc= nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "def prune_dataset(data_file):\n",
    "    data_dir = os.getcwd() + '/dataset/korean_unsmile_dataset/'\n",
    "    # data_dir = os.getcwd() + '/drive/MyDrive/dataset/korean_unsmile_dataset-main/'\n",
    "    df = pd.read_csv(data_dir + data_file, sep='\\t')\n",
    "\n",
    "    categories = df.columns.to_list()[1:]\n",
    "\n",
    "    for category in categories:\n",
    "        if category == 'clean':\n",
    "            continue\n",
    "        \n",
    "        indexes = list()\n",
    "        for i, _ in df.iterrows():\n",
    "            data = df.loc[i]\n",
    "            if data[category] == 1:\n",
    "                indexes.append(i)\n",
    "\n",
    "        mask = np.random.random(len(indexes)) > 0.5\n",
    "        indexes = mask * indexes\n",
    "        df.drop(indexes, inplace=True, errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def vectorize_dataset(df, vectorizer):\n",
    "    '''\n",
    "    transform '문장' column's elements from string to numpy array,\n",
    "    and return the pandas dataframe. Pororo is used for sentence embedding.\n",
    "    '''\n",
    "    \n",
    "    arr = []\n",
    "    sentence_col_idx = 2\n",
    "    sentence_col_name = df.columns.to_list()[sentence_col_idx]\n",
    "    \n",
    "    for i, _ in df.iterrows():\n",
    "        vectorized_sentence = vectorizer(df.loc[i][sentence_col_name])\n",
    "        arr.append(vectorized_sentence)\n",
    "\n",
    "    vectorized_sentence_col = pd.Series(arr, name=sentence_col_name)\n",
    "    df.drop(columns=sentence_col_name, axis=1, inplace=True) # remove a column with raw sentences\n",
    "    df.insert(sentence_col_idx, sentence_col_name, vectorized_sentence_col) # insert a new vectorized column\n",
    "    return df\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    '''\n",
    "    returns iterator for batch-size data.\n",
    "    \n",
    "    1. drop_last set to True: if the number of data is not divisible by batch size,\n",
    "    do not use the last batch whose size is smaller than batch size\n",
    "\n",
    "    2. shuffle set to True: shuffle dataset at every epoch\n",
    "    '''\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=dataset, batch_size=batch_size, drop_last=drop_last, shuffle=shuffle)\n",
    "  \n",
    "    for batch_dict in train_dataloader:\n",
    "        out_dict = {}\n",
    "        for k, v in batch_dict.items():\n",
    "            out_dict[k] = batch_dict[k].to(device)\n",
    "        yield out_dict\n",
    "\n",
    "def compute_metrics(y_pred, y_label, prev_precision):\n",
    "    '''\n",
    "    calculate precision and recall of batch-size data    \n",
    "    '''\n",
    "    \n",
    "    y_label = y_label.cpu()\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).cpu().int()\n",
    "    \n",
    "    if y_pred.sum().item() == 0:\n",
    "        precision = prev_precision\n",
    "    else:\n",
    "        precision = y_label[y_pred == 1].sum().item() / y_pred.sum().item()\n",
    "        \n",
    "    recall = y_label[y_pred == 1].sum().item() / (y_label == 1).sum().item()\n",
    "    return precision, recall\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'train_loss': [], 'test_loss': [],\n",
    "            'train_precision': [], 'test_precision': [],\n",
    "            'train_recall': [], 'test_recall': []}\n",
    "\n",
    "def count(df):\n",
    "    clean_data_num = df.sum()[-2]\n",
    "    hatred_data_num = df.sum()[1:].sum() - clean_data_num\n",
    "    print(f'hatred data: {hatred_data_num}, clean data: {clean_data_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd() + '/dataset/korean_unsmile_dataset/'\n",
    "train_data_file = 'unsmile_train_v1.0.tsv'\n",
    "test_data_file = 'unsmile_valid_v1.0.tsv'\n",
    "vectorizer = Pororo(task='sentence_embedding', lang='ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of train and test data\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "train_df = pd.read_csv(data_dir + train_data_file, sep='\\t')\n",
    "test_df = pd.read_csv(data_dir + test_data_file, sep='\\t')\n",
    "print(len(train_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate train and test data\n",
    "merged_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "print(f'number of data in merged dataframe, {len(merged_df)}')\n",
    "\n",
    "# remove data without label\n",
    "indexes = []\n",
    "for i in range(len(merged_df)):\n",
    "    data = merged_df.iloc[i]\n",
    "    if data[1:].sum() == 0:\n",
    "        indexes.append(i)\n",
    "\n",
    "merged_df.drop(indexes, inplace=True)\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "print(f'after removing not labeled data, {len(merged_df)}')\n",
    "merged_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "shuffled_df = merged_df.sample(frac=1)\n",
    "shuffled_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition data into 6 partitions(0 to 5).\n",
    "# each partition contains 3000 data\n",
    "p = -1\n",
    "n = 3000\n",
    "ids = []\n",
    "partitions = []\n",
    "\n",
    "for i in range(len(shuffled_df)):\n",
    "    if i % n == 0 and p < 5:\n",
    "        p += 1\n",
    "    ids.append(i)\n",
    "    partitions.append(p)\n",
    "\n",
    "ids = pd.Series(shuffled_df.index.values, name='id')\n",
    "partitions = pd.Series(partitions, name='partition')\n",
    "shuffled_df.reset_index(inplace=True)\n",
    "\n",
    "partitioned_df = pd.concat([ids, partitions, shuffled_df], axis=1)\n",
    "partitioned_df.drop(['index'], axis=1, inplace=True)\n",
    "partitioned_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize sentence using pororo\n",
    "vectorized_df = vectorize_dataset(partitioned_df.copy(), vectorizer)\n",
    "vectorized_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=30,\n",
    "    cuda=False,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6 # number of partitions\n",
    "partitions = list(range(K))\n",
    "high_prob_ids = defaultdict(list)\n",
    "\n",
    "for k in range(K):\n",
    "    # prepare data\n",
    "    test_partition = partitions[k: k + 1]\n",
    "    train_partition = partitions[0: k] + partitions[k + 1:]\n",
    "    test_df = vectorized_df.loc[vectorized_df['partition'].isin(test_partition)]\n",
    "    train_df = vectorized_df.loc[vectorized_df['partition'].isin(train_partition)]\n",
    "    test_data = UnsmileDataset(test_df)\n",
    "    train_data = UnsmileDataset(train_df)\n",
    "    \n",
    "    # initialize states during training\n",
    "    train_state = make_train_state(args)\n",
    "\n",
    "    # model\n",
    "    model = MultiLayerPerceptron()\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    # loss and optimizer\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # train model using 0,,k-1 and k+1,,5 partitions\n",
    "    start_time = time.time()\n",
    "    print(f'\\n<start training for {k}-partition!>')\n",
    "    print(f'-learning rate: {args.learning_rate}')\n",
    "    print(f'-total epochs: {args.num_epochs}')\n",
    "    print(f'-batch size: {args.batch_size}')\n",
    "    print(f\"-cuda {'avaialble' if args.cuda else 'not available'}\")\n",
    "\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        print(f'epoch{epoch_index + 1} : [', end='')\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_precision = 0.0\n",
    "        running_recall = 0.0\n",
    "        model.train() # this has effects on certain modules (ex, dropout)\n",
    "    \n",
    "        batch_generator = generate_batches(train_data, args.batch_size, device=args.device)\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            if batch_index % (int(int(len(train_data) / args.batch_size) / 20)) == 0:\n",
    "                print('>', end='')\n",
    "\n",
    "            x = batch_dict['x']\n",
    "            y = batch_dict['y']\n",
    "\n",
    "            # set all gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            y_pred = model(x)\n",
    "            loss = loss_func(y_pred, y.float())\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # metrics\n",
    "            loss_batch = loss.item()\n",
    "            running_loss += (loss_batch - running_loss) / (batch_index + 1) # moving average\n",
    "            batch_precision, batch_recall = compute_metrics(y_pred, y, running_precision)\n",
    "            running_precision += (batch_precision - running_precision) / (batch_index + 1)\n",
    "            running_recall += (batch_recall - running_recall) / (batch_index + 1)\n",
    "\n",
    "        print(']')\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_precision'].append(running_precision)\n",
    "        train_state['train_recall'].append(running_recall)\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        running_precision = 0.0\n",
    "        running_recall = 0.0\n",
    "        model.eval()\n",
    "\n",
    "        batch_generator = generate_batches(test_data, args.batch_size, device=args.device)\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            x = batch_dict['x']\n",
    "            y = batch_dict['y']\n",
    "            \n",
    "            # forward pass\n",
    "            y_pred = model(x)\n",
    "            loss = loss_func(y_pred, y.float())\n",
    "\n",
    "            # metrics\n",
    "            loss_batch = loss.item()\n",
    "            running_loss += (loss_batch - running_loss) / (batch_index + 1) # moving average\n",
    "            batch_precision, batch_recall = compute_metrics(y_pred, y, running_precision)\n",
    "            running_precision += (batch_precision - running_precision) / (batch_index + 1)\n",
    "            running_recall += (batch_recall - running_recall) / (batch_index + 1)\n",
    "    \n",
    "        train_state['test_loss'].append(running_loss)\n",
    "        train_state['test_precision'].append(running_precision)\n",
    "        train_state['test_recall'].append(running_recall)\n",
    "\n",
    "    print('<end training!>')\n",
    "    print('-results from the last epoch')\n",
    "    print(f'train/test precision: {train_state[\"train_precision\"][-1]}/{train_state[\"test_precision\"][-1]}')\n",
    "    print(f'train/test recall: {train_state[\"train_recall\"][-1]}/{train_state[\"test_recall\"][-1]}')\n",
    "    \n",
    "    # predict results for k partition using trained model\n",
    "    y_preds = []\n",
    "    y_labels = []\n",
    "    ids = []\n",
    "    threshold = 0.8\n",
    "    batch_generator = generate_batches(test_data, batch_size=1, device=args.device)\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        x = batch_dict['x']\n",
    "        y = batch_dict['y']\n",
    "        id = batch_dict['id']\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        y_pred = (torch.sigmoid(y_pred) > threshold).int()\n",
    "        y_preds.append(y_pred.detach().cpu().numpy().reshape(-1))\n",
    "        y_labels.append(y.detach().cpu().numpy().reshape(-1))\n",
    "        ids.append(id.detach().cpu().numpy().reshape(-1))\n",
    "        \n",
    "    y_preds = np.array(y_preds)\n",
    "    y_labels = np.array(y_labels)\n",
    "    ids = np.array(ids)\n",
    "    \n",
    "    for col_idx in range(11):\n",
    "        indexes = (y_preds[:, col_idx] == 1)  \n",
    "        high_prob_ids[col_idx].extend(ids[indexes].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itocol = {0: '여성/가족', 1: '남성', 2: '성소수자', 3: '인종/국적', 4: '연령',\n",
    "          5: '지역', 6: '종교', 7: '기타 혐오', 8: '악플/욕설', 9: 'clean', 10: '개인지칭'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# print(f'category: {itocol[i]}')\n",
    "# df = merged_df.loc[high_prob_ids[i]]\n",
    "# print(len(df.loc[df[itocol[i]] == 0]))\n",
    "# df.loc[df[itocol[i]] == 0]\n",
    "# # df.loc[df[itocol[i]] == 0, ['문장']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# print(f'category: {itocol[i]}')\n",
    "# df = merged_df.loc[high_prob_ids[i]]\n",
    "# print(len(df.loc[df[itocol[i]] == 0]))\n",
    "# df.loc[df[itocol[i]] == 0]\n",
    "# # df.loc[df[itocol[i]] == 0, ['문장']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 2\n",
    "# print(f'category: {itocol[i]}')\n",
    "# df = merged_df.loc[high_prob_ids[i]]\n",
    "# print(len(df.loc[df[itocol[i]] == 0]))\n",
    "# df.loc[df[itocol[i]] == 0]\n",
    "# # df.loc[df[itocol[i]] == 0, ['문장']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 3\n",
    "# print(f'category: {itocol[i]}')\n",
    "# df = merged_df.loc[high_prob_ids[i]]\n",
    "# print(len(df.loc[df[itocol[i]] == 0]))\n",
    "# df.loc[df[itocol[i]] == 0]\n",
    "# # df.loc[df[itocol[i]] == 0, ['문장']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 4\n",
    "# print(f'category: {itocol[i]}')\n",
    "# df = merged_df.loc[high_prob_ids[i]]\n",
    "# print(len(df.loc[df[itocol[i]] == 0]))\n",
    "# df.loc[df[itocol[i]] == 0]\n",
    "# # df.loc[df[itocol[i]] == 0, ['문장']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 5\n",
    "# print(f'category: {itocol[i]}')\n",
    "# df = merged_df.loc[high_prob_ids[i]]\n",
    "# print(len(df.loc[df[itocol[i]] == 0]))\n",
    "# df.loc[df[itocol[i]] == 0]\n",
    "# # df.loc[df[itocol[i]] == 0, ['문장']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 6\n",
    "# print(f'category: {itocol[i]}')\n",
    "# df = merged_df.loc[high_prob_ids[i]]\n",
    "# print(len(df.loc[df[itocol[i]] == 0]))\n",
    "# df.loc[df[itocol[i]] == 0]\n",
    "# # df.loc[df[itocol[i]] == 0, ['문장']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 7\n",
    "# print(f'category: {itocol[i]}')\n",
    "# df = merged_df.loc[high_prob_ids[i]]\n",
    "# print(len(df.loc[df[itocol[i]] == 0]))\n",
    "# df.loc[df[itocol[i]] == 0]\n",
    "# df.loc[df[itocol[i]] == 0, ['문장']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "print(f'category: {itocol[i]}')\n",
    "df = merged_df.loc[high_prob_ids[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "# df.loc[df[itocol[i]] == 0]\n",
    "# df.loc[df[itocol[i]] == 0, ['문장']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9\n",
    "print(f'category: {itocol[i]}')\n",
    "df = merged_df.loc[high_prob_ids[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "# df.loc[df[itocol[i]] == 0]\n",
    "# df.loc[df[itocol[i]] == 0, ['문장']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "print(f'category: {itocol[i]}')\n",
    "df = merged_df.loc[high_prob_ids[i]]\n",
    "print(len(df.loc[df[itocol[i]] == 0]))\n",
    "df.loc[df[itocol[i]] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = merged_df.columns[1:].to_list()\n",
    "to_category = {}\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    to_category[i] = categories[i]\n",
    "\n",
    "to_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revise_df = pd.read_table('./re-labeling.txt', sep='\\s+', header=None, names=['index', 'label'])\n",
    "print(len(revise_df))\n",
    "revise_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = list(revise_df['index'].values)\n",
    "labels = list(revise_df['label'].values)\n",
    "\n",
    "# one hot re-labeling\n",
    "for i, label in zip(indexes, labels):\n",
    "    for key in to_category:\n",
    "        if key == label:\n",
    "            merged_df.loc[i, to_category[key]] = 1\n",
    "        else:\n",
    "            merged_df.loc[i, to_category[key]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = merged_df.iloc[:, 0]\n",
    "label = merged_df.iloc[:, 1:]\n",
    "x_train, x_test, y_train, y_test = train_test_split(sentence, label, test_size=0.2, random_state=27)\n",
    "train_df = pd.concat([x_train, y_train], axis=1)\n",
    "test_df = pd.concat([x_test, y_test], axis=1)\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data distribution after cleaning\n",
    "categories = ['female', 'male', 'minor', 'race', 'age', 'region', 'religion', 'extra', 'curse', 'clean', 'individual']\n",
    "\n",
    "train_sum = train_df.sum()[1:]\n",
    "x1 = range(1, 2 * len(train_sum), 2)\n",
    "y1 = train_sum\n",
    "\n",
    "test_sum = test_df.sum()[1:]\n",
    "x2 = range(1, 2 * len(test_sum), 2)\n",
    "y2 = test_sum\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(x1, y1)\n",
    "plt.title('Train')\n",
    "plt.xticks(x1, categories)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(x2, y2)\n",
    "plt.title('Test')\n",
    "plt.xticks(x2, categories)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('-train')\n",
    "for i in y1.index:\n",
    "  print(f'{i}   ', end='')\n",
    "print()\n",
    "for i in y1:\n",
    "  print(f'{i}    ', end='')\n",
    "\n",
    "print()\n",
    "\n",
    "print('-test')\n",
    "for i in y2.index:\n",
    "  print(f'{i}   ', end='')\n",
    "print()\n",
    "for i in y2:\n",
    "  print(f'{i}     ', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd() + '/dataset/korean_unsmile_dataset/'\n",
    "train_df.to_csv(data_dir + 'cleaned_unsmile_train_v1.0.tsv', sep='\\t', header=True, index=False)\n",
    "test_df.to_csv(data_dir + 'cleaned_unsmile_valid_v1.0.tsv', sep='\\t', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e78d20c099c367000af7aff7232dce7210d5e30c41d5c8d2d0b0121a579d2b3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
