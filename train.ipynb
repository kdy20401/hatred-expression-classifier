{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pororo import Pororo\n",
    "\n",
    "\n",
    "class UnsmileDataset(Dataset):\n",
    "    '''\n",
    "    unsmile_df(pandas.DataFrame): unsmile dataset with vectorized sentence('문장')\n",
    "    '''\n",
    "    def __init__(self, unsmile_df):\n",
    "        self.unsmile_df = unsmile_df\n",
    "      \n",
    "    def __len__(self):\n",
    "        return self.unsmile_df.shape[0]\n",
    "      \n",
    "    def __getitem__(self, index):\n",
    "        sentence_vector = self.unsmile_df.iloc[index][0]\n",
    "        label_vector = self.unsmile_df.iloc[index][1:].to_numpy(dtype=np.int32) # note dtype\n",
    "        return sentence_vector, label_vector\n",
    "    \n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    '''\n",
    "    input: 768 dimension sentence vector transformed by Pororo sentence embedding \n",
    "    output: 11 dimension vector which contains values for '여성/가족', .... , '개인지칭'\n",
    "    '''\n",
    "    input_dim = 768\n",
    "    hidden_dim = 64\n",
    "    output_dim = 11  \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.fc= nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "def prune_dataset(data_file):\n",
    "    data_dir = os.getcwd() + '/dataset/korean_unsmile_dataset/'\n",
    "    # data_dir = os.getcwd() + '/drive/MyDrive/dataset/korean_unsmile_dataset-main/'\n",
    "    df = pd.read_csv(data_dir + data_file, sep='\\t')\n",
    "\n",
    "    categories = df.columns.to_list()[1:]\n",
    "\n",
    "    for category in categories:\n",
    "        if category == 'clean':\n",
    "            continue\n",
    "        \n",
    "        indexes = list()\n",
    "        for i, _ in df.iterrows():\n",
    "            data = df.loc[i]\n",
    "            if data[category] == 1:\n",
    "                indexes.append(i)\n",
    "\n",
    "        mask = np.random.random(len(indexes)) > 0.5\n",
    "        indexes = mask * indexes\n",
    "        df.drop(indexes, inplace=True, errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def vectorize_dataset(df, vectorizer):\n",
    "    '''\n",
    "    transform '문장' column's elements from string to numpy array,\n",
    "    and return the pandas dataframe. Pororo is used for sentence embedding.\n",
    "    '''\n",
    "    \n",
    "    arr = []\n",
    "    sentence_col = df.columns.to_list()[0]\n",
    "    \n",
    "    for i, _ in df.iterrows():\n",
    "        vectorized_sentence = vectorizer(df.loc[i][sentence_col])\n",
    "        arr.append(vectorized_sentence)\n",
    "\n",
    "    s = pd.Series(arr, name=sentence_col)\n",
    "    df.drop(columns=sentence_col, axis=1, inplace=True) # remove a column with raw sentences\n",
    "    return pd.concat([s, df], axis=1)\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    '''\n",
    "    returns iterator for batch-size data.\n",
    "    \n",
    "    1. drop_last set to True: if the number of data is not divisible by batch size,\n",
    "    do not use the last batch whose size is smaller than batch size\n",
    "\n",
    "    2. shuffle set to True: shuffle dataset at every epoch\n",
    "    '''\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=dataset, batch_size=batch_size, drop_last=drop_last, shuffle=shuffle)\n",
    "  \n",
    "    for sentences, labels in train_dataloader:\n",
    "        sentences = sentences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        yield sentences, labels\n",
    "\n",
    "def compute_metrics(y_pred, y_label):\n",
    "    '''\n",
    "    calculate accuracy and recall of batch-size data    \n",
    "    '''\n",
    "    \n",
    "    y_label = y_label.cpu()\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).cpu().int()\n",
    "    accuracy = (y_label == y_pred).sum().item() / (y_label.shape[0] * y_label.shape[1])\n",
    "    recall = y_label[y_pred == 1].sum().item() / (y_label == 1).sum().item()\n",
    "    return accuracy, recall\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'train_loss': [], 'test_loss': [],\n",
    "            'train_acc': [], 'test_acc': [],\n",
    "            'train_recall': [], 'test_recall': []}\n",
    "\n",
    "def count(df):\n",
    "    clean_data_num = df.sum()[-2]\n",
    "    hatred_data_num = df.sum()[1:].sum() - clean_data_num\n",
    "    print(f'hatred data: {hatred_data_num}, clean data: {clean_data_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_dir = os.getcwd() + '/drive/MyDrive/dataset/korean_unsmile_dataset-main/'\n",
    "data_dir = os.getcwd() + '/dataset/korean_unsmile_dataset/'\n",
    "train_data_file = 'unsmile_train_v1.0.tsv'\n",
    "test_data_file = 'unsmile_valid_v1.0.tsv'\n",
    "vectorizer = Pororo(task='sentence_embedding', lang='ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original implementation was that the sentence is vectorized dynamically,\n",
    "# but it spends quite a long time(about 150 sec) to vectorize one sentence.\n",
    "# so vectorize all sentences in advance and keep in memory.\n",
    "\n",
    "# 1. unbalanced dataset configuration\n",
    "train_df = pd.read_csv(data_dir + train_data_file, sep='\\t')\n",
    "test_df = pd.read_csv(data_dir + test_data_file, sep='\\t')\n",
    "vectorized_train_df = vectorize_dataset(train_df, vectorizer)\n",
    "vectorized_test_df = vectorize_dataset(test_df, vectorizer)\n",
    "train_data = UnsmileDataset(vectorized_train_df)\n",
    "test_data = UnsmileDataset(vectorized_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. balanced dataset. reduce the number of hatred data (hatred:clean = 3:2)\n",
    "pruned_train_df = prune_dataset('unsmile_train_v1.0.tsv')\n",
    "pruned_test_df = prune_dataset('unsmile_valid_v1.0.tsv')\n",
    "pruned_train_df.reset_index(drop=True, inplace=True) # to prevent NaNs after joining\n",
    "pruned_test_df.reset_index(drop=True, inplace=True)\n",
    "vectorized_pruned_train_df = vectorize_dataset(pruned_train_df, vectorizer)\n",
    "vectorized_pruned_test_df = vectorize_dataset(pruned_test_df, vectorizer)\n",
    "train_data = UnsmileDataset(vectorized_pruned_train_df)\n",
    "test_data = UnsmileDataset(vectorized_pruned_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. balanced dataset configuration. increase the number of clean data (hatred:clean = about 1:1)\n",
    "train_df = pd.read_csv(data_dir + train_data_file, sep='\\t')\n",
    "test_df = pd.read_csv(data_dir + test_data_file, sep='\\t')\n",
    "vectorized_train_df = vectorize_dataset(train_df, vectorizer)\n",
    "vectorized_test_df = vectorize_dataset(test_df, vectorizer)\n",
    "count(vectorized_train_df)\n",
    "\n",
    "clean_data_indexes = []\n",
    "for i in range(len(vectorized_train_df)):\n",
    "    data = vectorized_train_df.iloc[i]\n",
    "    if data['clean'] == 1:\n",
    "        clean_data_indexes.append(i)\n",
    "clean_df = vectorized_train_df.iloc[clean_data_indexes]\n",
    "copied_dfs = [clean_df.copy() for i in range(3)]\n",
    "ors_vectorized_train_df = pd.concat([vectorized_train_df, *copied_dfs], axis=0, ignore_index=True)\n",
    "count(ors_vectorized_train_df)\n",
    "train_data = UnsmileDataset(ors_vectorized_train_df)\n",
    "test_data = UnsmileDataset(vectorized_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    batch_size=128,\n",
    "    learning_rate=0.0005,\n",
    "    num_epochs=30,\n",
    "    cuda=False,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "\n",
    "# model\n",
    "model = MultiLayerPerceptron()\n",
    "model = model.to(args.device)\n",
    "\n",
    "# loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# train starts\n",
    "start_time = time.time()\n",
    "print(f'<training start!>')\n",
    "print(f'-total epochs: {args.num_epochs}')\n",
    "print(f'-batch size: {args.batch_size}')\n",
    "print(f'-learning rate: {args.learning_rate}')\n",
    "print(f\"-cuda {'avaialble' if args.cuda else 'not available'}\")\n",
    "    \n",
    "for epoch_index in range(args.num_epochs):\n",
    "    print(f'epoch{epoch_index + 1} : [', end='')\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    running_recall = 0.0\n",
    "    model.train() # this has effects on certain modules (ex, dropout)\n",
    "  \n",
    "    batch_generator = generate_batches(train_data, args.batch_size, device=args.device)\n",
    "\n",
    "    for batch_index, (x, y) in enumerate(batch_generator):\n",
    "        if batch_index % (int(int(len(train_data) / args.batch_size) / 20)) == 0:\n",
    "            print('>', end='')\n",
    "        \n",
    "        # set all gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y.float())\n",
    "    \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # metrics\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1) # moving average\n",
    "        batch_acc, batch_recall = compute_metrics(y_pred, y)\n",
    "        running_acc += (batch_acc - running_acc) / (batch_index + 1)\n",
    "        running_recall += (batch_recall - running_recall) / (batch_index + 1)\n",
    "    \n",
    "    print(']')\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "    train_state['train_recall'].append(running_recall)\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    running_recall = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    batch_generator = generate_batches(test_data, args.batch_size, device=args.device)\n",
    "    \n",
    "    for batch_index, (x, y) in enumerate(batch_generator):\n",
    "        # forward pass\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y.float())\n",
    "        \n",
    "        # metrics\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1) # moving average\n",
    "        batch_acc, batch_recall = compute_metrics(y_pred, y)\n",
    "        running_acc += (batch_acc - running_acc) / (batch_index + 1)\n",
    "        running_recall += (batch_recall - running_recall) / (batch_index + 1)\n",
    "  \n",
    "    train_state['test_loss'].append(running_loss)\n",
    "    train_state['test_acc'].append(running_acc)\n",
    "    train_state['test_recall'].append(running_recall)\n",
    "\n",
    "print(f'time flied: {start_time - time.time()}')\n",
    "print('<end training!>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(np.arange(0, args.num_epochs), train_state['train_loss'])\n",
    "plt.plot(np.arange(0, args.num_epochs), train_state['test_loss'])\n",
    "plt.legend(['train', 'test'])\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(np.arange(0, args.num_epochs), train_state['train_acc'])\n",
    "plt.plot(np.arange(0, args.num_epochs), train_state['test_acc'])\n",
    "plt.legend(['train', 'test'])\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Recall')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(np.arange(0, args.num_epochs), train_state['train_recall'])\n",
    "plt.plot(np.arange(0, args.num_epochs), train_state['test_recall'])\n",
    "plt.legend(['train', 'test'])\n",
    "\n",
    "plt.plot()\n",
    "\n",
    "print('-Final (moving average) values from the last epoch')\n",
    "print(f\"train loss: {train_state['train_loss'][-1]}\")\n",
    "print(f\"test loss: {train_state['test_loss'][-1]}\")\n",
    "print(f\"train accuracy: {train_state['train_acc'][-1]}\")\n",
    "print(f\"test accuracy: {train_state['test_acc'][-1]}\")\n",
    "print(f\"train recall: {train_state['train_recall'][-1]}\")\n",
    "print(f\"test recall: {train_state['test_recall'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total test data metrics: accuracy and recall\n",
    "batch_generator = generate_batches(test_data, 1, device=args.device)\n",
    "\n",
    "y_preds = []\n",
    "y_labels = []\n",
    "\n",
    "for batch_index, (x, y) in enumerate(batch_generator):\n",
    "    y_pred = model(x)\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).int()\n",
    "    y_preds.append(y_pred.detach().cpu().numpy().reshape(-1))\n",
    "    y_labels.append(y.detach().cpu().numpy().reshape(-1))\n",
    "\n",
    "y_preds = np.array(y_preds)\n",
    "y_labels = np.array(y_labels)\n",
    "\n",
    "# accuracy\n",
    "accuracy = (y_preds == y_labels).sum() / (y_preds.shape[0] * y_preds.shape[1])\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "for i in range(11):\n",
    "    \n",
    "    pred_col = y_preds[:, i]\n",
    "    label_col = y_labels[:, i]\n",
    "    \n",
    "    # precision\n",
    "    TP_plus_FP = pred_col[pred_col == 1].sum()\n",
    "    TP = (label_col[pred_col == 1] == 1).sum()\n",
    "    \n",
    "    if TP_plus_FP == 0:\n",
    "        precisions.append(-1)  \n",
    "    else:\n",
    "        precisions.append(TP / TP_plus_FP)\n",
    "   \n",
    "    # recall\n",
    "    TP_plus_FN = (label_col[label_col == 1]).sum()\n",
    "    recalls.append(TP / TP_plus_FN)\n",
    "    \n",
    "print('-Results of test data using a trained model\\ncategory\\t\\tprecision\\t\\trecall')\n",
    "for category, precision, recall in zip(vectorized_train_df.columns[1:], precisions, recalls):\n",
    "    print(f'{category}\\t\\t{precision}\\t\\t{recall}')\n",
    "\n",
    "print(f'\\naverage precision: {np.average(precisions)}')\n",
    "print(f'average recall: {np.average(recalls)}\\n')\n",
    "\n",
    "recall_dict = {}\n",
    "for category, recall in zip(vectorized_train_df.columns[1:], recalls):\n",
    "    recall_dict[category] = recall\n",
    "print('-Sorted by recall\\ncategory\\t\\trecall')\n",
    "for category, recall in sorted(recall_dict.items(), key=lambda x: x[1]):\n",
    "    print(f'{category}\\t\\t{recall}')\n",
    "\n",
    "\n",
    "# '개인지칭' 과 '기타 혐오' 의 성능이 매우 낮음,, 왜?\n",
    "# 데이터 통계를 보면, '개인지칭', '기타 혐오', '연령' 순서대로 데이터 수가 가장 적다. -> 적은 데이터 수가 문제?\n",
    "# 그런데 '연령' 보다 '악플/욕설' 데이터 수가 약 5배 많음에도 불구하고, recall 은 큰 차이 안남,,\n",
    "# 반대로, recall 이 가장 높은 '지역' 이나 '종교' 데이터 수가 많은 것도 아님.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Baseline\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "     여성/가족       0.85      0.70      0.76       394\n",
    "         남성       0.87      0.83      0.85       334\n",
    "      성소수자       0.90      0.78      0.83       280\n",
    "     인종/국적       0.87      0.79      0.82       426\n",
    "         연령       0.92      0.75      0.83       146\n",
    "         지역       0.87      0.88      0.88       260\n",
    "         종교       0.87      0.86      0.87       290\n",
    "      기타혐오       0.92      0.18      0.30       134\n",
    "     악플/욕설       0.76      0.59      0.67       786\n",
    "       clean       0.74      0.79      0.77       935\n",
    "\n",
    "    micro avg      0.82      0.73      0.77      3985\n",
    "    macro avg      0.86      0.72      0.76      3985\n",
    "    weighted avg   0.82      0.73      0.77      3985\n",
    "    samples avg    0.76      0.74      0.75      3985\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e78d20c099c367000af7aff7232dce7210d5e30c41d5c8d2d0b0121a579d2b3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
