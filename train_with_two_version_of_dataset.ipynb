{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pororo import Pororo\n",
    "\n",
    "\n",
    "class UnsmileDataset(Dataset):\n",
    "    '''\n",
    "    unsmile_df(pandas.DataFrame): unsmile dataset with vectorized sentence('문장')\n",
    "    '''\n",
    "    def __init__(self, unsmile_df):\n",
    "        self.unsmile_df = unsmile_df\n",
    "      \n",
    "    def __len__(self):\n",
    "        return self.unsmile_df.shape[0]\n",
    "      \n",
    "    def __getitem__(self, index):\n",
    "        sentence_vector = self.unsmile_df.iloc[index][0]\n",
    "        label_vector = self.unsmile_df.iloc[index][1:].to_numpy(dtype=np.int32) # note dtype\n",
    "        return sentence_vector, label_vector\n",
    "    \n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    '''\n",
    "    input: 768 dimension sentence vector transformed by Pororo sentence embedding \n",
    "    output: 11 dimension vector which contains values for '여성/가족', .... , '개인지칭'\n",
    "    '''\n",
    "    input_dim = 768\n",
    "    hidden_dim = 512\n",
    "    output_dim = 11  \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.fc= nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "def prune_dataset(data_file):\n",
    "    data_dir = os.getcwd() + '/dataset/korean_unsmile_dataset/'\n",
    "    # data_dir = os.getcwd() + '/drive/MyDrive/dataset/korean_unsmile_dataset-main/'\n",
    "    df = pd.read_csv(data_dir + data_file, sep='\\t')\n",
    "\n",
    "    categories = df.columns.to_list()[1:]\n",
    "\n",
    "    for category in categories:\n",
    "        if category == 'clean':\n",
    "            continue\n",
    "        \n",
    "        indexes = list()\n",
    "        for i, _ in df.iterrows():\n",
    "            data = df.loc[i]\n",
    "            if data[category] == 1:\n",
    "                indexes.append(i)\n",
    "\n",
    "        mask = np.random.random(len(indexes)) > 0.5\n",
    "        indexes = mask * indexes\n",
    "        df.drop(indexes, inplace=True, errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def vectorize_dataset(df, vectorizer):\n",
    "    '''\n",
    "    transform '문장' column's elements from string to numpy array,\n",
    "    and return the pandas dataframe. Pororo is used for sentence embedding.\n",
    "    '''\n",
    "    \n",
    "    arr = []\n",
    "    sentence_col = df.columns.to_list()[0]\n",
    "    \n",
    "    for i, _ in df.iterrows():\n",
    "        vectorized_sentence = vectorizer(df.loc[i][sentence_col])\n",
    "        arr.append(vectorized_sentence)\n",
    "\n",
    "    s = pd.Series(arr, name=sentence_col)\n",
    "    df.drop(columns=sentence_col, axis=1, inplace=True) # remove a column with raw sentences\n",
    "    return pd.concat([s, df], axis=1)\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    '''\n",
    "    returns iterator for batch-size data.\n",
    "    \n",
    "    1. drop_last set to True: if the number of data is not divisible by batch size,\n",
    "    do not use the last batch whose size is smaller than batch size\n",
    "\n",
    "    2. shuffle set to True: shuffle dataset at every epoch\n",
    "    '''\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=dataset, batch_size=batch_size, drop_last=drop_last, shuffle=shuffle)\n",
    "  \n",
    "    for sentences, labels in train_dataloader:\n",
    "        sentences = sentences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        yield sentences, labels\n",
    "\n",
    "def compute_metrics(y_pred, y_label, prev_precision):\n",
    "    '''\n",
    "    calculate precision and recall of batch-size data    \n",
    "    '''\n",
    "    \n",
    "    y_label = y_label.cpu()\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).cpu().int()\n",
    "    \n",
    "    if y_pred.sum().item() == 0:\n",
    "        precision = prev_precision\n",
    "    else:\n",
    "        precision = y_label[y_pred == 1].sum().item() / y_pred.sum().item()\n",
    "        \n",
    "    recall = y_label[y_pred == 1].sum().item() / (y_label == 1).sum().item()\n",
    "    return precision, recall\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'train_loss': [], 'test_loss': [],\n",
    "            'train_precision': [], 'test_precision': [],\n",
    "            'train_recall': [], 'test_recall': []}\n",
    "\n",
    "def count(df):\n",
    "    clean_data_num = df.sum()[-2]\n",
    "    hatred_data_num = df.sum()[1:].sum() - clean_data_num\n",
    "    print(f'hatred data: {hatred_data_num}, clean data: {clean_data_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd() + '/dataset/korean_unsmile_dataset/'\n",
    "train_data_file = 'unsmile_train_v1.0.tsv'\n",
    "test_data_file = 'unsmile_valid_v1.0.tsv'\n",
    "processed_train_data_file = 'processed_unsmile_train_v1.0.tsv'\n",
    "processed_test_data_file = 'processed_unsmile_valid_v1.0.tsv'\n",
    "\n",
    "vectorizer = Pororo(task='sentence_embedding', lang='ko')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare original data vs cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_dir + train_data_file, sep='\\t')\n",
    "test_df = pd.read_csv(data_dir + test_data_file, sep='\\t')\n",
    "processed_train_df = pd.read_csv(data_dir + processed_train_data_file, sep='\\t')\n",
    "processed_test_df = pd.read_csv(data_dir + processed_test_data_file, sep='\\t')\n",
    "\n",
    "# the original implementation was that the sentence is vectorized dynamically,\n",
    "# but it spends quite a long time(about 150 sec) to vectorize one sentence.\n",
    "# so vectorize all sentences in advance and keep in memory.\n",
    "vectorized_train_df = vectorize_dataset(train_df, vectorizer)\n",
    "vectorized_test_df = vectorize_dataset(test_df, vectorizer)\n",
    "vectorized_processed_train_df = vectorize_dataset(processed_train_df, vectorizer)\n",
    "vectorized_processed_test_df = vectorize_dataset(processed_test_df, vectorizer)\n",
    "\n",
    "# original data\n",
    "ud_train = UnsmileDataset(vectorized_train_df)\n",
    "ud_test = UnsmileDataset(vectorized_test_df)\n",
    "\n",
    "# preprocessed data\n",
    "ud_train1 = UnsmileDataset(vectorized_processed_train_df)\n",
    "ud_test1 = UnsmileDataset(vectorized_processed_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select original dataset or cleaned dataset\n",
    "train_data = ud_train1\n",
    "test_data = ud_test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model using one of two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<training start!>\n",
      "-learning rate: 0.001\n",
      "-total epochs: 30\n",
      "-batch size: 128\n",
      "-cuda avaialble\n",
      "epoch 1 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 2 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 3 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 4 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 5 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 6 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 7 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 8 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 9 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 10 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 11 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 12 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 13 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 14 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 15 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 16 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 17 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 18 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 19 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 20 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 21 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 22 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 23 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 24 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 25 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 26 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 27 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 28 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 29 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 30 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "<training at 0-th iteration ends!>\n",
      "epoch 1 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 2 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 3 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 4 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 5 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 6 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 7 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 8 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 9 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 10 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 11 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 12 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 13 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 14 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 15 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 16 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 17 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 18 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 19 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 20 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 21 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 22 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 23 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 24 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 25 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 26 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 27 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 28 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 29 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 30 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "<training at 1-th iteration ends!>\n",
      "epoch 1 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 2 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 3 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 4 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 5 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 6 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 7 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 8 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 9 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 10 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 11 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 12 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 13 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 14 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 15 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 16 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 17 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 18 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 19 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 20 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 21 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 22 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 23 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 24 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 25 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 26 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 27 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 28 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 29 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 30 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "<training at 2-th iteration ends!>\n",
      "epoch 1 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 2 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 3 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 4 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 5 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 6 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 7 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 8 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 9 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 10 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 11 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 12 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 13 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 14 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 15 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 16 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 17 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 18 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 19 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 20 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 21 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 22 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 23 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 24 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 25 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 26 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 27 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 28 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 29 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 30 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "<training at 3-th iteration ends!>\n",
      "epoch 1 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 2 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 3 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 4 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 5 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 6 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 7 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 8 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 9 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 10 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 11 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 12 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 13 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 14 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 15 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 16 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 17 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 18 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 19 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 20 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 21 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 22 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 23 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 24 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 25 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 26 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 27 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 28 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 29 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch 30 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "<training at 4-th iteration ends!>\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=30,\n",
    "    cuda=False,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "\n",
    "# model\n",
    "model = MultiLayerPerceptron()\n",
    "model = model.to(args.device)\n",
    "\n",
    "# loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# train starts\n",
    "start_time = time.time()\n",
    "print(f'<training start!>')\n",
    "print(f'-learning rate: {args.learning_rate}')\n",
    "print(f'-total epochs: {args.num_epochs}')\n",
    "print(f'-batch size: {args.batch_size}')\n",
    "print(f\"-cuda {'avaialble' if args.cuda else 'not available'}\")\n",
    "\n",
    "itocol = {0: '여성/가족', 1: '남성', 2: '성소수자', 3: '인종/국적', 4: '연령',\n",
    "          5: '지역', 6: '종교', 7: '기타 혐오', 8: '악플/욕설', 9: '깨끗', 10: '개인지칭'}\n",
    "\n",
    "precisions_per_category = defaultdict(list)\n",
    "recalls_per_category = defaultdict(list)\n",
    "f1_scores_per_category = defaultdict(list)\n",
    "\n",
    "total_precisions = []\n",
    "total_recalls = []\n",
    "total_f1_scores = []\n",
    "\n",
    "iterations = 5\n",
    "\n",
    "for k in range(iterations):\n",
    "    # train model\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        print(f'epoch{epoch_index + 1: 2} : [', end='')\n",
    "\n",
    "        model.train() # this has effects on certain modules (ex, dropout)\n",
    "\n",
    "        batch_generator = generate_batches(train_data, args.batch_size, device=args.device)\n",
    "\n",
    "        for batch_index, (x, y) in enumerate(batch_generator):\n",
    "            if batch_index % (int(int(len(train_data) / args.batch_size) / 20)) == 0:\n",
    "                print('>', end='')\n",
    "\n",
    "            # set all gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            y_pred = model(x)\n",
    "            loss = loss_func(y_pred, y.float())\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(']')\n",
    "    \n",
    "    # test model\n",
    "    model.eval()\n",
    "\n",
    "    batch_generator = generate_batches(test_data, 1, device=args.device)\n",
    "    y_preds = []\n",
    "    y_labels = []\n",
    "\n",
    "    for batch_index, (x, y) in enumerate(batch_generator):\n",
    "        y_pred = (torch.sigmoid(model(x)) > 0.5).int()\n",
    "        y_preds.append(y_pred.detach().cpu().numpy().reshape(-1))\n",
    "        y_labels.append(y.detach().cpu().numpy().reshape(-1))\n",
    "\n",
    "    y_preds = np.array(y_preds)\n",
    "    y_labels = np.array(y_labels)\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    col_num = len(vectorized_train_df.columns[1:])\n",
    "    \n",
    "    # precision, recall and f1 score for each category\n",
    "    for i in range(col_num):\n",
    "        pred_col = y_preds[:, i]\n",
    "        label_col = y_labels[:, i]\n",
    "    \n",
    "        # precision and recall\n",
    "        precision = (label_col[pred_col == 1] == 1).sum() / pred_col.sum()\n",
    "        recall = (label_col[pred_col == 1] == 1).sum() / label_col.sum()\n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "        \n",
    "        precisions_per_category[i].append(precision)\n",
    "        recalls_per_category[i].append(recall)\n",
    "        f1_scores_per_category[i].append(f1_score)\n",
    "        \n",
    "    # precision, recall and f1 score for overall data\n",
    "    _precision = y_labels[y_preds == 1].sum()/ y_preds.sum()\n",
    "    _recall = y_labels[y_preds == 1].sum() / y_labels.sum()\n",
    "    _f1_score = 2 * _precision * _recall / (_precision + _recall)\n",
    "    \n",
    "    total_precisions.append(_precision)\n",
    "    total_recalls.append(_recall)\n",
    "    total_f1_scores.append(_f1_score)\n",
    "\n",
    "    print(f'<training at {k}-th iteration ends!>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Test data result of each category\n",
      "category      precision recall f1 score\n",
      "여성/가족     \t 0.738\t 0.528\t 0.613\n",
      "남성        \t 0.797\t 0.589\t 0.677\n",
      "성소수자      \t 0.877\t 0.629\t 0.732\n",
      "인종/국적     \t 0.779\t 0.550\t 0.644\n",
      "연령        \t 0.827\t 0.447\t 0.579\n",
      "지역        \t 0.847\t 0.691\t 0.761\n",
      "종교        \t 0.884\t 0.763\t 0.818\n",
      "기타 혐오     \t 0.618\t 0.130\t 0.213\n",
      "악플/욕설     \t 0.635\t 0.375\t 0.470\n",
      "깨끗        \t 0.705\t 0.612\t 0.655\n",
      "개인지칭      \t 0.752\t 0.092\t 0.162\n",
      "\n",
      "-Overall metrics\n",
      "average precision:  0.752\n",
      "average recall:  0.536\n",
      "average f1_score:  0.626\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "print('-Test data result of each category\\ncategory      precision recall f1 score')\n",
    "for i in itocol:\n",
    "    category = itocol[i]\n",
    "    avg_precision = np.average(precisions_per_category[i])\n",
    "    avg_recall = np.average(recalls_per_category[i])\n",
    "    avg_f1_score = np.average(f1_scores_per_category[i])\n",
    "    print(f'{category:10}\\t{avg_precision: .3f}\\t{avg_recall: .3f}\\t{avg_f1_score: .3f}')\n",
    "\n",
    "print('\\n-Overall metrics')\n",
    "print(f'average precision: {np.average(total_precisions): .3f}')\n",
    "print(f'average recall: {np.average(total_recalls): .3f}')\n",
    "print(f'average f1_score: {np.average(total_f1_scores): .3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
