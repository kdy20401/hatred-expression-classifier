{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pororo import Pororo\n",
    "\n",
    "\n",
    "class UnsmileDataset(Dataset):\n",
    "    '''\n",
    "    unsmile_df(pandas.DataFrame): unsmile dataset with vectorized sentence('문장')\n",
    "    '''\n",
    "    def __init__(self, unsmile_df):\n",
    "        self.unsmile_df = unsmile_df\n",
    "      \n",
    "    def __len__(self):\n",
    "        return self.unsmile_df.shape[0]\n",
    "      \n",
    "    def __getitem__(self, index):\n",
    "        sentence_vector = self.unsmile_df.iloc[index][0]\n",
    "        label_vector = self.unsmile_df.iloc[index][1:].to_numpy(dtype=np.int32) # note dtype\n",
    "        return sentence_vector, label_vector\n",
    "    \n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    '''\n",
    "    input: 768 dimension sentence vector transformed by Pororo sentence embedding \n",
    "    output: 11 dimension vector which contains values for '여성/가족', .... , '개인지칭'\n",
    "    '''\n",
    "    input_dim = 768\n",
    "    hidden_dim = 512\n",
    "    output_dim = 11  \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.fc= nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "def prune_dataset(data_file):\n",
    "    data_dir = os.getcwd() + '/dataset/korean_unsmile_dataset/'\n",
    "    # data_dir = os.getcwd() + '/drive/MyDrive/dataset/korean_unsmile_dataset-main/'\n",
    "    df = pd.read_csv(data_dir + data_file, sep='\\t')\n",
    "\n",
    "    categories = df.columns.to_list()[1:]\n",
    "\n",
    "    for category in categories:\n",
    "        if category == 'clean':\n",
    "            continue\n",
    "        \n",
    "        indexes = list()\n",
    "        for i, _ in df.iterrows():\n",
    "            data = df.loc[i]\n",
    "            if data[category] == 1:\n",
    "                indexes.append(i)\n",
    "\n",
    "        mask = np.random.random(len(indexes)) > 0.5\n",
    "        indexes = mask * indexes\n",
    "        df.drop(indexes, inplace=True, errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def vectorize_dataset(df, vectorizer):\n",
    "    '''\n",
    "    transform '문장' column's elements from string to numpy array,\n",
    "    and return the pandas dataframe. Pororo is used for sentence embedding.\n",
    "    '''\n",
    "    \n",
    "    arr = []\n",
    "    sentence_col = df.columns.to_list()[0]\n",
    "    \n",
    "    for i, _ in df.iterrows():\n",
    "        vectorized_sentence = vectorizer(df.loc[i][sentence_col])\n",
    "        arr.append(vectorized_sentence)\n",
    "\n",
    "    s = pd.Series(arr, name=sentence_col)\n",
    "    df.drop(columns=sentence_col, axis=1, inplace=True) # remove a column with raw sentences\n",
    "    return pd.concat([s, df], axis=1)\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    '''\n",
    "    returns iterator for batch-size data.\n",
    "    \n",
    "    1. drop_last set to True: if the number of data is not divisible by batch size,\n",
    "    do not use the last batch whose size is smaller than batch size\n",
    "\n",
    "    2. shuffle set to True: shuffle dataset at every epoch\n",
    "    '''\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=dataset, batch_size=batch_size, drop_last=drop_last, shuffle=shuffle)\n",
    "  \n",
    "    for sentences, labels in train_dataloader:\n",
    "        sentences = sentences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        yield sentences, labels\n",
    "\n",
    "def compute_metrics(y_pred, y_label, prev_precision):\n",
    "    '''\n",
    "    calculate precision and recall of batch-size data    \n",
    "    '''\n",
    "    \n",
    "    y_label = y_label.cpu()\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).cpu().int()\n",
    "    \n",
    "    if y_pred.sum().item() == 0:\n",
    "        precision = prev_precision\n",
    "    else:\n",
    "        precision = y_label[y_pred == 1].sum().item() / y_pred.sum().item()\n",
    "        \n",
    "    recall = y_label[y_pred == 1].sum().item() / (y_label == 1).sum().item()\n",
    "    return precision, recall\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'train_loss': [], 'test_loss': [],\n",
    "            'train_precision': [], 'test_precision': [],\n",
    "            'train_recall': [], 'test_recall': []}\n",
    "\n",
    "def count(df):\n",
    "    clean_data_num = df.sum()[-2]\n",
    "    hatred_data_num = df.sum()[1:].sum() - clean_data_num\n",
    "    print(f'hatred data: {hatred_data_num}, clean data: {clean_data_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd() + '/dataset/korean_unsmile_dataset/'\n",
    "train_data_file = 'unsmile_train_v1.0.tsv'\n",
    "test_data_file = 'unsmile_valid_v1.0.tsv'\n",
    "processed_train_data_file = 'cleaned_unsmile_train_v1.0.tsv'\n",
    "processed_test_data_file = 'cleaned_unsmile_valid_v1.0.tsv'\n",
    "\n",
    "vectorizer = Pororo(task='sentence_embedding', lang='ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_dir + train_data_file, sep='\\t')\n",
    "test_df = pd.read_csv(data_dir + test_data_file, sep='\\t')\n",
    "\n",
    "# the original implementation was that the sentence is vectorized dynamically,\n",
    "# but it spends quite a long time(about 150 sec) to vectorize one sentence.\n",
    "# so vectorize all sentences in advance and keep in memory.\n",
    "vectorized_train_df = vectorize_dataset(train_df, vectorizer)\n",
    "vectorized_test_df = vectorize_dataset(test_df, vectorizer)\n",
    "\n",
    "# original data\n",
    "train_data = UnsmileDataset(vectorized_train_df)\n",
    "test_data = UnsmileDataset(vectorized_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=30,\n",
    "    cuda=False,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "\n",
    "# model\n",
    "model = MultiLayerPerceptron()\n",
    "model = model.to(args.device)\n",
    "\n",
    "# loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# train starts\n",
    "start_time = time.time()\n",
    "print(f'<training start!>')\n",
    "print(f'-learning rate: {args.learning_rate}')\n",
    "print(f'-total epochs: {args.num_epochs}')\n",
    "print(f'-batch size: {args.batch_size}')\n",
    "print(f\"-cuda {'avaialble' if args.cuda else 'not available'}\")\n",
    "    \n",
    "for epoch_index in range(args.num_epochs):\n",
    "    print(f'epoch{epoch_index + 1} : [', end='')\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_precision = 0.0\n",
    "    running_recall = 0.0\n",
    "    model.train() # this has effects on certain modules (ex, dropout)\n",
    "  \n",
    "    batch_generator = generate_batches(train_data, args.batch_size, device=args.device)\n",
    "\n",
    "    for batch_index, (x, y) in enumerate(batch_generator):\n",
    "        if batch_index % (int(int(len(train_data) / args.batch_size) / 20)) == 0:\n",
    "            print('>', end='')\n",
    "        \n",
    "        # set all gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y.float())\n",
    "    \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # metrics\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1) # moving average\n",
    "        batch_precision, batch_recall = compute_metrics(y_pred, y, running_precision)\n",
    "        running_precision += (batch_precision - running_precision) / (batch_index + 1)\n",
    "        running_recall += (batch_recall - running_recall) / (batch_index + 1)\n",
    "    \n",
    "    print(']')\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_precision'].append(running_precision)\n",
    "    train_state['train_recall'].append(running_recall)\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_precision = 0.0\n",
    "    running_recall = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    batch_generator = generate_batches(test_data, args.batch_size, device=args.device)\n",
    "    \n",
    "    for batch_index, (x, y) in enumerate(batch_generator):\n",
    "        # forward pass\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y.float())\n",
    "        \n",
    "        # metrics\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1) # moving average\n",
    "        batch_precision, batch_recall = compute_metrics(y_pred, y, running_precision)\n",
    "        running_precision += (batch_precision - running_precision) / (batch_index + 1)\n",
    "        running_recall += (batch_recall - running_recall) / (batch_index + 1)\n",
    "  \n",
    "    train_state['test_loss'].append(running_loss)\n",
    "    train_state['test_precision'].append(running_precision)\n",
    "    train_state['test_recall'].append(running_recall)\n",
    "\n",
    "print(f'time flied: {time.time() - start_time} sec')\n",
    "print('<end training!>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of performance during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(np.arange(0, args.num_epochs), train_state['train_loss'])\n",
    "plt.plot(np.arange(0, args.num_epochs), train_state['test_loss'])\n",
    "plt.legend(['train', 'test'])\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(np.arange(0, args.num_epochs), train_state['train_precision'])\n",
    "plt.plot(np.arange(0, args.num_epochs), train_state['test_precision'])\n",
    "plt.legend(['train', 'test'])\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Recall')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(np.arange(0, args.num_epochs), train_state['train_recall'])\n",
    "plt.plot(np.arange(0, args.num_epochs), train_state['test_recall'])\n",
    "plt.legend(['train', 'test'])\n",
    "\n",
    "plt.plot()\n",
    "\n",
    "print('-Final (moving average) values from the last epoch')\n",
    "print(f\"train loss: {train_state['train_loss'][-1]}\")\n",
    "print(f\"test loss: {train_state['test_loss'][-1]}\")\n",
    "print(f\"train precision: {train_state['train_precision'][-1]}\")\n",
    "print(f\"test precision: {train_state['test_precision'][-1]}\")\n",
    "print(f\"train recall: {train_state['train_recall'][-1]}\")\n",
    "print(f\"test recall: {train_state['test_recall'][-1]}\")\n",
    "\n",
    "\n",
    "train_data_generator = generate_batches(train_data, 1, device=args.device)\n",
    "test_data_generator = generate_batches(test_data, 1, device=args.device)\n",
    "\n",
    "train_loss = 0\n",
    "test_loss = 0 \n",
    "\n",
    "for index, (x, y) in enumerate(train_data_generator):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_func(y_pred, y.float())\n",
    "    train_loss += loss.item()\n",
    "train_loss /= len(train_data)\n",
    "\n",
    "\n",
    "for index, (x, y) in enumerate(test_data_generator):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_func(y_pred, y.float())\n",
    "    test_loss += loss.item()\n",
    "test_loss /= len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total test data metrics: precision and recall\n",
    "batch_generator = generate_batches(test_data, 1, device=args.device)\n",
    "\n",
    "y_preds = []\n",
    "y_labels = []\n",
    "\n",
    "for batch_index, (x, y) in enumerate(batch_generator):\n",
    "    y_pred = model(x)\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).int()\n",
    "    y_preds.append(y_pred.detach().cpu().numpy().reshape(-1))\n",
    "    y_labels.append(y.detach().cpu().numpy().reshape(-1))\n",
    "\n",
    "y_preds = np.array(y_preds)\n",
    "y_labels = np.array(y_labels)\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "col_num = len(vectorized_train_df.columns[1:])\n",
    "\n",
    "for i in range(col_num):\n",
    "    pred_col = y_preds[:, i]\n",
    "    label_col = y_labels[:, i]\n",
    "    \n",
    "    # precision and recall\n",
    "    if pred_col.sum() == 0:\n",
    "        precision = -1 # undefined\n",
    "    else:\n",
    "        precision = (label_col[pred_col == 1] == 1).sum() / pred_col.sum()\n",
    "    recall = (label_col[pred_col == 1] == 1).sum() / label_col.sum()\n",
    "    \n",
    "    precisions.append(precision)    \n",
    "    recalls.append(recall)\n",
    "    \n",
    "print('-Test data result of each category\\ncategory\\t\\tprecision\\t\\trecall')\n",
    "for category, precision, recall in zip(vectorized_train_df.columns[1:], precisions, recalls):\n",
    "    if precision == -1:\n",
    "        print(f'{category}\\t\\tundefined\\t\\t{recall}')\n",
    "    else:\n",
    "        print(f'{category}\\t\\t{precision}\\t\\t{recall}')\n",
    "\n",
    "\n",
    "recall_dict = {}\n",
    "for category, recall in zip(vectorized_train_df.columns[1:], recalls):\n",
    "    recall_dict[category] = recall\n",
    "print('\\n-Sorted by recall\\ncategory\\t\\trecall')\n",
    "for category, recall in sorted(recall_dict.items(), key=lambda x: x[1]):\n",
    "    print(f'{category}\\t\\t{recall}')\n",
    "\n",
    "    \n",
    "# total precision and recall\n",
    "total_precision = y_labels[y_preds == 1].sum() / y_preds.sum()\n",
    "total_recall = y_labels[y_preds == 1].sum() / y_labels.sum()\n",
    "print(f'\\n-Total performance')\n",
    "print(f'precision: {total_precision}')\n",
    "print(f'recall: {total_recall}')\n",
    "print(f'f1 score: {2 * total_precision * total_recall / (total_precision + total_recall)}')\n",
    "\n",
    "# '개인지칭' 과 '기타 혐오' 의 성능이 매우 낮음,, 왜?\n",
    "# 데이터 통계를 보면, '개인지칭', '기타 혐오', '연령' 순서대로 데이터 수가 가장 적다. -> 적은 데이터 수가 문제?\n",
    "# 그런데 '연령' 보다 '악플/욕설' 데이터 수가 약 5배 많음에도 불구하고, recall 은 큰 차이 안남,,\n",
    "# 반대로, recall 이 가장 높은 '지역' 이나 '종교' 데이터 수가 많은 것도 아님."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline performance provided by Smilegate\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "     여성/가족      0.85      0.70      0.76       394\n",
    "         남성      0.87      0.83      0.85       334\n",
    "      성소수자      0.90      0.78      0.83       280\n",
    "     인종/국적      0.87      0.79      0.82       426\n",
    "         연령      0.92      0.75      0.83       146\n",
    "         지역      0.87      0.88      0.88       260\n",
    "         종교      0.87      0.86      0.87       290\n",
    "      기타혐오      0.92      0.18      0.30       134\n",
    "     악플/욕설      0.76      0.59      0.67       786\n",
    "       clean      0.74      0.79      0.77       935\n",
    "\n",
    "    micro avg      0.82      0.73      0.77      3985\n",
    "    macro avg      0.86      0.72      0.76      3985\n",
    "    weighted avg   0.82      0.73      0.77      3985\n",
    "    samples avg    0.76      0.74      0.75      3985\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e78d20c099c367000af7aff7232dce7210d5e30c41d5c8d2d0b0121a579d2b3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
