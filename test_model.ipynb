{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pororo import Pororo\n",
    "\n",
    "\n",
    "class UnsmileDataset(Dataset):\n",
    "    '''\n",
    "    unsmile_df(pandas.DataFrame): unsmile dataset with vectorized sentence('문장')\n",
    "    '''\n",
    "    def __init__(self, unsmile_df):\n",
    "        self.unsmile_df = unsmile_df\n",
    "      \n",
    "    def __len__(self):\n",
    "        return self.unsmile_df.shape[0]\n",
    "      \n",
    "    def __getitem__(self, index):\n",
    "        sentence_vector = self.unsmile_df.iloc[index][0]\n",
    "        label_vector = self.unsmile_df.iloc[index][1:].to_numpy(dtype=np.int32) # note dtype\n",
    "        return sentence_vector, label_vector\n",
    "    \n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    '''\n",
    "    input: 768 dimension sentence vector transformed by Pororo sentence embedding \n",
    "    output: 11 dimension vector which contains values for '여성/가족', .... , '개인지칭'\n",
    "    '''\n",
    "    input_dim = 768\n",
    "    hidden_dim = 512\n",
    "    output_dim = 11  \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.fc= nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "def prune_dataset(data_file):\n",
    "    data_dir = os.getcwd() + '/dataset/korean_unsmile_dataset/'\n",
    "    # data_dir = os.getcwd() + '/drive/MyDrive/dataset/korean_unsmile_dataset-main/'\n",
    "    df = pd.read_csv(data_dir + data_file, sep='\\t')\n",
    "\n",
    "    categories = df.columns.to_list()[1:]\n",
    "\n",
    "    for category in categories:\n",
    "        if category == 'clean':\n",
    "            continue\n",
    "        \n",
    "        indexes = list()\n",
    "        for i, _ in df.iterrows():\n",
    "            data = df.loc[i]\n",
    "            if data[category] == 1:\n",
    "                indexes.append(i)\n",
    "\n",
    "        mask = np.random.random(len(indexes)) > 0.5\n",
    "        indexes = mask * indexes\n",
    "        df.drop(indexes, inplace=True, errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def vectorize_dataset(df, vectorizer):\n",
    "    '''\n",
    "    transform '문장' column's elements from string to numpy array,\n",
    "    and return the pandas dataframe. Pororo is used for sentence embedding.\n",
    "    '''\n",
    "    \n",
    "    arr = []\n",
    "    sentence_col = df.columns.to_list()[0]\n",
    "    \n",
    "    for i, _ in df.iterrows():\n",
    "        vectorized_sentence = vectorizer(df.loc[i][sentence_col])\n",
    "        arr.append(vectorized_sentence)\n",
    "\n",
    "    s = pd.Series(arr, name=sentence_col)\n",
    "    df.drop(columns=sentence_col, axis=1, inplace=True) # remove a column with raw sentences\n",
    "    return pd.concat([s, df], axis=1)\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    '''\n",
    "    returns iterator for batch-size data.\n",
    "    \n",
    "    1. drop_last set to True: if the number of data is not divisible by batch size,\n",
    "    do not use the last batch whose size is smaller than batch size\n",
    "\n",
    "    2. shuffle set to True: shuffle dataset at every epoch\n",
    "    '''\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=dataset, batch_size=batch_size, drop_last=drop_last, shuffle=shuffle)\n",
    "  \n",
    "    for sentences, labels in train_dataloader:\n",
    "        sentences = sentences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        yield sentences, labels\n",
    "\n",
    "def compute_metrics(y_pred, y_label, prev_precision):\n",
    "    '''\n",
    "    calculate precision and recall of batch-size data    \n",
    "    '''\n",
    "    \n",
    "    y_label = y_label.cpu()\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).cpu().int()\n",
    "    \n",
    "    if y_pred.sum().item() == 0:\n",
    "        precision = prev_precision\n",
    "    else:\n",
    "        precision = y_label[y_pred == 1].sum().item() / y_pred.sum().item()\n",
    "        \n",
    "    recall = y_label[y_pred == 1].sum().item() / (y_label == 1).sum().item()\n",
    "    return precision, recall\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'train_loss': [], 'test_loss': [],\n",
    "            'train_precision': [], 'test_precision': [],\n",
    "            'train_recall': [], 'test_recall': []}\n",
    "\n",
    "def count(df):\n",
    "    clean_data_num = df.sum()[-2]\n",
    "    hatred_data_num = df.sum()[1:].sum() - clean_data_num\n",
    "    print(f'hatred data: {hatred_data_num}, clean data: {clean_data_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd() + '/dataset/korean_unsmile_dataset/'\n",
    "train_data_file = 'unsmile_train_v1.0.tsv'\n",
    "test_data_file = 'unsmile_valid_v1.0.tsv'\n",
    "processed_train_data_file = 'cleaned_unsmile_train_v1.0.tsv'\n",
    "processed_test_data_file = 'cleaned_unsmile_valid_v1.0.tsv'\n",
    "\n",
    "vectorizer = Pororo(task='sentence_embedding', lang='ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_dir + train_data_file, sep='\\t')\n",
    "test_df = pd.read_csv(data_dir + test_data_file, sep='\\t')\n",
    "processed_train_df = pd.read_csv(data_dir + processed_train_data_file, sep='\\t')\n",
    "processed_test_df = pd.read_csv(data_dir + processed_test_data_file, sep='\\t')\n",
    "\n",
    "# the original implementation was that the sentence is vectorized dynamically,\n",
    "# but it spends quite a long time(about 150 sec) to vectorize one sentence.\n",
    "# so vectorize all sentences in advance and keep in memory.\n",
    "vectorized_train_df = vectorize_dataset(train_df, vectorizer)\n",
    "vectorized_test_df = vectorize_dataset(test_df, vectorizer)\n",
    "vectorized_processed_train_df = vectorize_dataset(processed_train_df, vectorizer)\n",
    "vectorized_processed_test_df = vectorize_dataset(processed_test_df, vectorizer)\n",
    "\n",
    "# original data\n",
    "ud_train = UnsmileDataset(vectorized_train_df)\n",
    "ud_test = UnsmileDataset(vectorized_test_df)\n",
    "\n",
    "# preprocessed data\n",
    "ud_train1 = UnsmileDataset(vectorized_processed_train_df)\n",
    "ud_test1 = UnsmileDataset(vectorized_processed_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select original dataset or cleaned dataset\n",
    "train_data = ud_train\n",
    "test_data = ud_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<training start!>\n",
      "-learning rate: 0.001\n",
      "-total epochs: 30\n",
      "-batch size: 128\n",
      "-cuda avaialble\n",
      "epoch1 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch2 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch3 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch4 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch5 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch6 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch7 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch8 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch9 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch10 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch11 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch12 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch13 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch14 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch15 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch16 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch17 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch18 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch19 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch20 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch21 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch22 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch23 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch24 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch25 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch26 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch27 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch28 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch29 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "epoch30 : [>>>>>>>>>>>>>>>>>>>>>>>>]\n",
      "time flied: 168.55926322937012 sec\n",
      "<end training!>\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=30,\n",
    "    cuda=False,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "\n",
    "# model\n",
    "model = MultiLayerPerceptron()\n",
    "model = model.to(args.device)\n",
    "\n",
    "# loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# train starts\n",
    "start_time = time.time()\n",
    "print(f'<training start!>')\n",
    "print(f'-learning rate: {args.learning_rate}')\n",
    "print(f'-total epochs: {args.num_epochs}')\n",
    "print(f'-batch size: {args.batch_size}')\n",
    "print(f\"-cuda {'avaialble' if args.cuda else 'not available'}\")\n",
    "    \n",
    "for epoch_index in range(args.num_epochs):\n",
    "    print(f'epoch{epoch_index + 1} : [', end='')\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_precision = 0.0\n",
    "    running_recall = 0.0\n",
    "    model.train() # this has effects on certain modules (ex, dropout)\n",
    "  \n",
    "    batch_generator = generate_batches(train_data, args.batch_size, device=args.device)\n",
    "\n",
    "    for batch_index, (x, y) in enumerate(batch_generator):\n",
    "        if batch_index % (int(int(len(train_data) / args.batch_size) / 20)) == 0:\n",
    "            print('>', end='')\n",
    "        \n",
    "        # set all gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y.float())\n",
    "    \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # metrics\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1) # moving average\n",
    "        batch_precision, batch_recall = compute_metrics(y_pred, y, running_precision)\n",
    "        running_precision += (batch_precision - running_precision) / (batch_index + 1)\n",
    "        running_recall += (batch_recall - running_recall) / (batch_index + 1)\n",
    "    \n",
    "    print(']')\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_precision'].append(running_precision)\n",
    "    train_state['train_recall'].append(running_recall)\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_precision = 0.0\n",
    "    running_recall = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    batch_generator = generate_batches(test_data, args.batch_size, device=args.device)\n",
    "    \n",
    "    for batch_index, (x, y) in enumerate(batch_generator):\n",
    "        # forward pass\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y.float())\n",
    "        \n",
    "        # metrics\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1) # moving average\n",
    "        batch_precision, batch_recall = compute_metrics(y_pred, y, running_precision)\n",
    "        running_precision += (batch_precision - running_precision) / (batch_index + 1)\n",
    "        running_recall += (batch_recall - running_recall) / (batch_index + 1)\n",
    "  \n",
    "    train_state['test_loss'].append(running_loss)\n",
    "    train_state['test_precision'].append(running_precision)\n",
    "    train_state['test_recall'].append(running_recall)\n",
    "\n",
    "print(f'time flied: {time.time() - start_time} sec')\n",
    "print('<end training!>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "'''\n",
    " precisions_per_categories = {0: [70, 71, ,,, 75]\n",
    "                              1: [50, 55, ,,, 54]]}\n",
    "'''\n",
    "\n",
    "itocol = {0: '여성/가족', 1: '남성', 2: '성소수자', 3: '인종/국적', 4: '연령',\n",
    "          5: '지역', 6: '종교', 7: '기타 혐오', 8: '악플/욕설', 9: '깨끗', 10: '개인지칭'}\n",
    "\n",
    "precisions_per_categories = defaultdict(list)\n",
    "recalls_per_categories = defaultdict(list)\n",
    "f1_scores_per_categories = defaultdict(list)\n",
    "\n",
    "total_precisions = []\n",
    "total_recalls = []\n",
    "total_f1_scores = []\n",
    "\n",
    "iterations = 5\n",
    "\n",
    "for i in range(iterations):\n",
    "    batch_generator = generate_batches(test_data, 1, device=args.device)\n",
    "\n",
    "    y_preds = []\n",
    "    y_labels = []\n",
    "\n",
    "    for batch_index, (x, y) in enumerate(batch_generator):\n",
    "        y_pred = model(x)\n",
    "        y_pred = (torch.sigmoid(y_pred) > 0.5).int()\n",
    "        y_preds.append(y_pred.detach().cpu().numpy().reshape(-1))\n",
    "        y_labels.append(y.detach().cpu().numpy().reshape(-1))\n",
    "\n",
    "    y_preds = np.array(y_preds)\n",
    "    y_labels = np.array(y_labels)\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    col_num = len(vectorized_train_df.columns[1:])\n",
    "    \n",
    "    # precision, recall and f1 score for each category\n",
    "    for i in range(col_num):\n",
    "        pred_col = y_preds[:, i]\n",
    "        label_col = y_labels[:, i]\n",
    "    \n",
    "        # precision and recall\n",
    "        precision = (label_col[pred_col == 1] == 1).sum() / pred_col.sum()\n",
    "        recall = (label_col[pred_col == 1] == 1).sum() / label_col.sum()\n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "        \n",
    "        precisions_per_categories[i].append(precision)\n",
    "        recalls_per_categories[i].append(recall)\n",
    "        f1_scores_per_categories[i].append(f1_score)\n",
    "        \n",
    "    # precision, recall and f1 score for overall data\n",
    "    precision = y_labels[y_preds == 1].sum()/ y_preds.sum()\n",
    "    recall = y_labels[y_preds == 1].sum() / y_labels.sum()\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    total_precisions.append(precision)\n",
    "    total_recalls.append(recall)\n",
    "    total_f1_scores.append(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Test data result of each category\n",
      "category      precision recall f1 score\n",
      "여성/가족 \t 0.74\t 0.55\t 0.63\n",
      "남성    \t 0.77\t 0.56\t 0.65\n",
      "성소수자  \t 0.89\t 0.64\t 0.74\n",
      "인종/국적 \t 0.76\t 0.61\t 0.68\n",
      "연령    \t 0.84\t 0.42\t 0.56\n",
      "지역    \t 0.84\t 0.71\t 0.77\n",
      "종교    \t 0.87\t 0.78\t 0.82\n",
      "기타 혐오 \t 0.75\t 0.13\t 0.23\n",
      "악플/욕설 \t 0.63\t 0.38\t 0.47\n",
      "깨끗    \t 0.73\t 0.59\t 0.65\n",
      "개인지칭  \t 0.67\t 0.08\t 0.14\n",
      "\n",
      "-Overall metrics\n",
      "average precision:  0.76\n",
      "average recall:  0.54\n",
      "average f1_score:  0.63\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "print('-Test data result of each category\\ncategory      precision recall f1 score')\n",
    "for i in itocol:\n",
    "    category = itocol[i]\n",
    "    avg_precision = np.average(precisions_per_categories[i])\n",
    "    avg_recall = np.average(recalls_per_categories[i])\n",
    "    avg_f1_score = np.average(f1_scores_per_categories[i])\n",
    "    print(f'{category:6}\\t{avg_precision: .2f}\\t{avg_recall: .2f}\\t{avg_f1_score: .2f}')\n",
    "\n",
    "print('\\n-Overall metrics')\n",
    "print(f'average precision: {np.average(total_precisions): .2f}')\n",
    "print(f'average recall: {np.average(total_recalls): .2f}')\n",
    "print(f'average f1_score: {np.average(total_f1_scores): .2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
